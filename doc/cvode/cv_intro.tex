%===================================================================================
\chapter{Introduction}\label{s:intro}
%===================================================================================

{\cvode} is part of a software family called {\sundials}:
SUite of Nonlinear and DIfferential/ALgebraic equation Solvers~\cite{HBGLSSW:05}.
This suite consists of {\cvode}, {\arkode}, {\kinsol}, and {\ida}, and variants
of these with sensitivity analysis capabilities.
%
%---------------------------------
\section{Historical Background}\label{ss:history}
%---------------------------------

\index{CVODE@{\cvode}!relationship to {\vode}, {\vodpk}|(}
{\F} solvers for ODE initial value problems are widespread and heavily used.
Two solvers that have been written at LLNL in the past are {\vode}~\cite{BBH:89}
and {\vodpk}~\cite{Byr:92}.
{\vode}\index{VODE@{\vode}} is a general purpose solver that includes methods for
both stiff and nonstiff systems, and in the stiff case uses direct methods (full or
banded) for the solution of the linear systems that arise at each implicit
step. Externally, {\vode} is very similar to the well known solver
{\lsode}\index{LSODE@{\lsode}}~\cite{RaHi:94}. {\vodpk}\index{VODPK@{\vodpk}}
is a variant of {\vode} that uses a preconditioned Krylov (iterative)
method, namely GMRES, for the solution of the linear systems. {\vodpk}
is a powerful tool for large stiff systems because it combines
established methods for stiff integration, nonlinear iteration, and
Krylov (linear) iteration with a problem-specific treatment of the
dominant source of stiffness, in the form of the user-supplied
preconditioner matrix~\cite{BrHi:89}.  The capabilities of both
{\vode} and {\vodpk} have been combined in the {\CC}-language package
{\cvode}\index{CVODE@{\cvode}}~\cite{CoHi:96}.

At present, {\cvode} may utilize a variety of Krylov methods provided
in {\sundials} that can be used in conjuction with Newton iteration:
these include the GMRES (Generalized Minimal RESidual)~\cite{SaSc:86},
FGMRES (Flexible Generalized Minimum RESidual)~\cite{Saa:93},
Bi-CGStab (Bi-Conjugate Gradient Stabilized)~\cite{Van:92}, TFQMR
(Transpose-Free Quasi-Minimal Residual)~\cite{Fre:93}, and PCG
(Preconditioned Conjugate Gradient)~\cite{HeSt:52} linear iterative
methods.  As Krylov methods, these require almost no
matrix storage for solving the Newton equations as compared to direct
methods. However, the algorithms allow for a user-supplied preconditioner
matrix, and for most problems preconditioning is essential for an
efficient solution.
For very large stiff ODE systems, the Krylov methods are preferable over
direct linear solver methods, and are often the only feasible choice.
Among the Krylov methods in {\sundials}, we recommend GMRES as the
best overall choice.  However, users are encouraged to compare all
options, especially if encountering convergence failures with GMRES.
Bi-CGStab and TFQMR have an advantage in storage requirements, in
that the number of workspace vectors they require is fixed, while that
number for GMRES depends on the desired Krylov subspace size.  FGMRES
has an advantage in that it is designed to support preconditioners
that vary between iterations (e.g.~iterative methods).  PCG exhibits
rapid convergence and minimal workspace vectors, but only works for
symmetric linear systems.

In the process of translating the {\vode} and {\vodpk} algorithms into
{\CC}, the overall {\cvode} organization has been changed considerably.
One key feature of the {\cvode} organization is that the linear system
solvers comprise a layer of code modules that is separated from the
integration algorithm, allowing for easy modification and expansion of
the linear solver array.  A second key feature is a separate module
devoted to vector operations; this facilitated the extension to
multiprosessor environments with minimal impacts on the rest of the
solver, resulting in {\pvode}\index{PVODE@{\pvode}}~\cite{ByHi:99},
the parallel variant of {\cvode}.  \index{CVODE@{\cvode}!relationship
to {\vode}, {\vodpk}|)}

\index{CVODE@{\cvode}!relationship to {\cvode}, {\pvode}|(} Around 2002,
the functionality of {\cvode} and {\pvode} were combined into one
single code, simply called {\cvode}. Development of this version of
{\cvode} was concurrent with a redesign of the vector operations
module across the {\sundials} suite. The key feature of the
{\nvector} module is that it is written in terms of abstract vector
operations with the actual vector kernels attached by a particular
implementation (such as serial or parallel) of {\nvector}. This allows
writing the {\sundials} solvers in a manner independent of the actual
{\nvector} implementation (which can be user-supplied), as well as
allowing more than one {\nvector} module linked into an executable file.
{\sundials} (and thus {\cvode}) is supplied with six different {\nvector}
implementations:
serial, MPI-parallel, and both OpenMP and Pthreads thread-parallel
{\nvector} implementations, a Hypre parallel implementation,
and a PETSc implementation.
\index{CVODE@{\cvode}!relationship to {\cvode}, {\pvode}|)}

\index{CVODE@{\cvode}!motivation for writing in C|(}
There are several motivations for choosing the {\CC} language for {\cvode}.
First, a general movement away from {\F} and toward {\CC} in scientific
computing was apparent.  Second, the pointer, structure, and dynamic
memory allocation features in C are extremely useful in software of
this complexity, with the great variety of method options offered.
Finally, we prefer {\CC} over {\CPP} for {\cvode} because of the wider
availability of {\CC} compilers, the potentially greater efficiency of {\CC},
and the greater ease of interfacing the solver to applications written
in extended {\F}.
\index{CVODE@{\cvode}!motivation for writing in C|)}

\section{Changes from previous versions}

\subsection*{Changes in v5.0.0}

\subsubsection*{Build system changes}

\begin{itemize}
\item Increased the minimum required CMake version to 3.5 for most {\sundials}
configurations, and 3.10 when CUDA or OpenMP with device offloading are enabled.
%
\item The CMake option \id{BLAS\_ENABLE} and the variable \id{BLAS\_LIBRARIES} have
been removed to simplify builds as {\sundials} packages do not use BLAS
directly. For third party libraries that require linking to BLAS, the path to
the BLAS library should be included in the \id{\_LIBRARIES} variable for the
third party library \textit{e.g.}, \id{SUPERLUDIST\_LIBRARIES} when enabling
SuperLU\_DIST.
%
\item Fixed a bug in the build system that prevented the {\nvecpthreads} module from
being built.
\end{itemize}

\subsubsection*{NVECTOR module changes}

\begin{itemize}
\item Two new functions were added to aid in creating custom {\nvector} objects. The
constructor \id{N\_VNewEmpty} allocates an ``empty'' generic {\nvector} with the
object's content pointer and the function pointers in the operations structure
initialized to \id{NULL}. When used in the constructor for custom objects this
function will ease the introduction of any new optional operations to the
{\nvector} API by ensuring only required operations need to be set.
Additionally, the function \id{N\_VCopyOps(w, v)} has been added to copy the
operation function pointers between vector objects. When used in clone routines
for custom vector objects these functions also will ease the introduction of
any new optional operations to the {\nvector} API by ensuring all operations
are copied when cloning objects. See \S\ref{ss:nvecutils} for more details.
%
\item Two new {\nvector} implementations, {\nvecmanyvector} and
{\nvecmpimanyvector}, have been created to support flexible partitioning
of solution data among different processing elements (e.g., CPU + GPU) or for
multi-physics problems that couple distinct MPI-based simulations together. This
implementation is accompanied by additions to user documentation and {\sundials}
examples. See \S\ref{ss:nvec_manyvector} and \S\ref{ss:nvec_mpimanyvector} for
more details.
%
\item One new requried vector operation and ten new optional vector operations have
been added to the {\nvector} API. The new required operation, \id{N\_VGetLength},
returns the global length of an \id{N\_Vector}. The optional operations have
been added to support the new \newline\noindent
{\nvecmpimanyvector} implementation. The
operation \id{N\_VGetCommunicator} must be implemented by subvectors that are
combined to create an {\nvecmpimanyvector}, but is not used outside of
this context. The remaining nine operations are optional local reduction
operations intended to eliminate unnecessary latency when performing vector
reduction operations (norms, etc.) on distributed memory systems. The optional
local reduction vector operations are
\id{N\_VDotProdLocal},
\id{N\_VMaxNormLocal},
\id{N\_VMinLocal},
\id{N\_VL1NormLocal},
\id{N\_VWSqrSumLocal},
\id{N\_VWSqrSumMaskLocal},
\id{N\_VInvTestLocal},
\id{N\_VConstrMaskLocal}, and
\id{N\_VMinQuotientLocal}.
If an {\nvector} implementation defines any of the local operations as
\id{NULL}, then the {\nvecmpimanyvector} will call standard {\nvector}
operations to complete the computation. See \S\ref{ss:nveclocalops} for more
details.
%
\item An additional {\nvector} implementation, {\nvecmpiplusx}, has been created to
support the MPI+X paradigm where X is a type of on-node parallelism
(\textit{e.g.}, OpenMP, CUDA). The implementation is accompanied by additions to
user documentation and {\sundials} examples. See \S\ref{ss:nvec_mpiplusx} for
more details.
%
\item The \id{*\_MPICuda} and \id{*\_MPIRaja} functions have been removed from the
{\nveccuda} and {\nvecraja} implementations respectively. Accordingly, the
\id{nvector\_mpicuda.h}, \newline\noindent
\id{nvector\_mpiraja.h},
\id{libsundials\_nvecmpicuda.lib}, and \id{libsundials\_nvecmpicudaraja.lib}
files have been removed. Users should use the {\nvecmpiplusx} module coupled
in conjunction with the {\nveccuda} or {\nvecraja} modules to replace the
functionality. The necessary changes are minimal and should require few
code modifications.
%
\item Fixed a memory leak in the {\nvecpetsc} module clone function.
%
\item Made performance improvements to the {\nveccuda} module. Users who utilize a
non-default stream should no longer see default stream synchronizations
after memory transfers.
%
\item Added a new constructor to the {\nveccuda} module that allows a user to provide
custom allocate and free functions for the vector data array and internal
reduction buffer. See \S\ref{ss:nvec_cuda_functions} for more details.
%
\item Added new Fortran 2003 interfaces for most {\nvector} modules. See Chapter
\ref{s:nvector} for more details on how to use the interfaces.
%
\item Added three new {\nvector} utility functions,
\id{FN\_VGetVecAtIndexVectorArray},\newline\noindent
\id{FN\_VSetVecAtIndexVectorArray}, and
\id{FN\_VNewVectorArray},
for working with \id{N\_Vector} arrays when using the Fortran 2003 interfaces.
See \S\ref{ss:nvecutils} for more details.
\end{itemize}

\subsubsection*{SUNMatrix module changes}

\begin{itemize}
\item Two new functions were added to aid in creating custom {\sunmatrix} objects. The
constructor \id{SUNMatNewEmpty} allocates an ``empty'' generic {\sunmatrix} with
the object's content pointer and the function pointers in the operations
structure initialized to \id{NULL}. When used in the constructor for custom
objects this function will ease the introduction of any new optional operations
to the {\sunmatrix} API by ensuring only required operations need to be set.
Additionally, the function \id{SUNMatCopyOps(A, B)} has been added to copy the
operation function pointers between matrix objects. When used in clone routines
for custom matrix objects these functions also will ease the introduction of any
new optional operations to the {\sunmatrix} API by ensuring all operations are
copied when cloning objects. See \S\ref{ss:sunmatrix_utilities} for more details.
%
\item A new operation, \id{SUNMatMatvecSetup}, was added to the {\sunmatrix} API.
Users who have implemented custom {\sunmatrix} modules will need to at least
update their code to set the corresponding \id{ops} structure member,
\id{matvecsetup}, to \id{NULL}. See \S\ref{ss:sunmatrix_functions} for more
details.
%
\item The generic {\sunmatrix} API now defines error codes to be returned by
{\sunmatrix} operations. Operations which return an integer flag indiciating
success/failure may return different values than previously. See
\S\ref{ss:sunmatrix_ReturnCodes} for more details.
%
\item A new {\sunmatrix} (and {\sunlinsol}) implementation was added to
facilitate the use of the SuperLU\_DIST library with {\sundials}. See
\S\ref{ss:sunmat_slunrloc} for more details.
%
\item Added new Fortran 2003 interfaces for most {\sunmatrix} modules. See Chapter
\ref{s:sunmatrix} for more details on how to use the interfaces.
\end{itemize}

\subsubsection*{SUNLinearSolver module changes}

\begin{itemize}
\item A new function was added to aid in creating custom {\sunlinsol} objects.
The constructor \id{SUNLinSolNewEmpty} allocates an ``empty'' generic
{\sunlinsol} with the object's content pointer and the function pointers
in the operations structure initialized to \id{NULL}. When used in the
constructor for custom objects this function will ease the introduction of any
new optional operations to the {\sunlinsol} API by ensuring only required
operations need to be set. See \S\ref{ss:sunlinsol_custom} for more details.
%
\item The return type of the {\sunlinsol} API function \id{SUNLinSolLastFlag}
has changed from \id{long int} to \id{sunindextype} to be consistent with the
type used to store row indices in dense and banded linear solver modules.
%
\item Added a new optional operation to the {\sunlinsol} API,
\id{SUNLinSolGetID}, that returns a \newline\noindent
\id{SUNLinearSolver\_ID} for identifying the
linear solver module.
%
\item The {\sunlinsol} API has been updated to make the initialize and setup
functions optional.
%
\item A new {\sunlinsol} (and {\sunmatrix}) implementation was added to
facilitate the use of the SuperLU\_DIST library with {\sundials}. See
\S\ref{ss:sunlinsol_sludist} for more details.
%
\item Added a new {\sunlinsol} implementation,
\id{SUNLinearSolver\_cuSolverSp\_batchQR}, which leverages the NVIDIA cuSOLVER
sparse batched QR method for efficiently solving block diagonal linear systems
on NVIDIA GPUs. See \S\ref{ss:sunlinsol_cuspbqr} for more details.
%
\item Added three new accessor functions to the {\sunlinsolklu} module,
\id{SUNLinSol\_KLUGetSymbolic},
\id{SUNLinSol\_KLUGetNumeric}, and
\id{SUNLinSol\_KLUGetCommon},
to provide user access to the underlying KLU solver structures. See
\S\ref{ss:sunlinsol_klu_functions} for more details.
%
\item Added new Fortran 2003 interfaces for most {\sunlinsol} modules. See
Chapter \ref{s:sunlinsol} for more details on how to use the interfaces.
\end{itemize}

\subsubsection*{SUNNonlinearSolver module changes}

\begin{itemize}
\item A new function was added to aid in creating custom {\sunnonlinsol}
objects. The constructor \id{SUNNonlinSolNewEmpty} allocates an ``empty''
generic {\sunnonlinsol} with the object's content pointer and the function
pointers in the operations structure initialized to \id{NULL}. When used in the
constructor for custom objects this function will ease the introduction of any
new optional operations to the {\sunnonlinsol} API by ensuring only
required operations need to be set. See \S\ref{ss:sunnonlinsol_custom} for more
details.
%
\item To facilitate the use of user supplied nonlinear solver convergence test
functions the \newline\noindent
\id{SUNNonlinSolSetConvTestFn} function in the
{\sunnonlinsol} API has been updated to take a \id{void*} data pointer as
input. The supplied data pointer will be passed to the nonlinear solver
convergence test function on each call.
%
\item The inputs values passed to the first two inputs of the \id{SUNNonlinSolSolve}
function in the {\sunnonlinsol} have been chagned to be the predicted
state and the initial guess for the correction to that state. Additionally,
the definitions of \id{SUNNonlinSolLSetupFn} and \id{SUNNonlinSolLSolveFn} in
the {\sunnonlinsol} API have been updated to remove unused input
parameters. For more information on the nonlinear system formulation see
\S\ref{s:sunnonlinsol_interface} and for more details on the API functions see
Chapter \ref{c:sunnonlinsol}.
%
\item Added a new {\sunnonlinsol} implementation, {\sunnonlinsolpetsc}, which
interfaces to the PETSc SNES nonlinear solver API. See
\S\ref{s:sunnonlinsolpetsc} for more details.
%
\item Added new Fortran 2003 interfaces for most {\sunnonlinsol} modules. See
Chapter \ref{c:sunnonlinsol} for more details on how to use the interfaces.
\end{itemize}

\subsubsection*{CVODE changes}

\begin{itemize}
\item Fixed a bug in the {\cvode} constraint handling where the step size could be
set below the minimum step size.
%
\item Fixed a bug in the {\cvode} nonlinear solver interface where the norm of the
accumulated correction was not updated when using a non-default convergence
test function.
%
\item Fixed a memeory leak in {\fcvode} when not using the default nonlinear solver.
%
\item Removed extraneous calls to \id{N\_VMin} for simulations where the scalar valued
absolute tolerance, or all entries of the vector-valued absolute tolerance
array, are strictly positive.  In this scenario, {\cvode} will remove at least
one global reduction per time step.
%
\item The CVLS interface has been updated to only zero the Jacobian matrix before
calling a user-supplied Jacobian evaluation function when the attached linear
solver has type \newline\noindent
\id{SUNLINEARSOLVER\_DIRECT}.
%
\item A new linear solver interface function \id{CVLsLinSysFn} was added as an
alternative method for evaluating the linear system $M = I - \gamma J$.
%
\item Added two new functions, \id{CVodeGetCurrentGamma} and
\id{CVodeGetCurrentState}, which may be useful to users who choose to provide
their own nonlinear solver implementations.
%
\item The {\cvode} Fortran 2003 interface was completely redone to be more sustainable
and to allow users to write more idiomatic Fortran. See Chapter \ref{s:cvfort}
for more details.
\end{itemize}


\subsection*{Changes in v4.1.0}

An additional {\nvector} implementation was added for the
{\tpetra} vector from the {\trilinos} library to facilitate interoperability
between {\sundials} and {\trilinos}. This implementation is accompanied by
additions to user documentation and {\sundials} examples.

A bug was fixed where a nonlinear solver object could be freed twice in some use
cases.

The \id{EXAMPLES\_ENABLE\_RAJA} CMake option has been removed. The option \id{EXAMPLES\_ENABLE\_CUDA}
enables all examples that use CUDA including the RAJA examples with a CUDA back end (if the RAJA
{\nvector} is enabled).

The implementation header file \id{cvode\_impl.h} is no longer installed. This means users
who are directly manipulating the \id{CVodeMem} structure will need to update their code
to use {\cvode}'s public API.

Python is no longer required to run \id{make test} and \id{make test\_install}.

\subsection*{Changes in v4.0.2}

Added information on how to contribute to {\sundials} and a contributing agreement.

Moved definitions of DLS and SPILS backwards compatibility functions to a source file.
The symbols are now included in the {\cvode} library, \id{libsundials\_cvode}.

\subsection*{Changes in v4.0.1}

No changes were made in this release.

\subsection*{Changes in v4.0.0}

{\cvode}'s previous direct and iterative linear solver interfaces,
{\cvdls} and {\cvspils}, have been merged into a single unified linear
solver interface, {\cvls}, to support any valid {\sunlinsol} module.
This includes the ``DIRECT'' and ``ITERATIVE'' types as well as the new
``MATRIX\_ITERATIVE'' type. Details regarding how {\cvls} utilizes linear
solvers of each type as well as discussion regarding intended use cases for
user-supplied {\sunlinsol} implementations are included in
Chapter~\ref{s:sunlinsol}. All {\cvode} example programs and the standalone
linear solver examples have been updated to use the unified linear solver
interface.

The unified interface for the new {\cvls} module is very similar to the
previous {\cvdls} and {\cvspils} interfaces. To minimize challenges in user
migration to the new names, the previous {\CC} and {\F} routine names may still
be used; these will be deprecated in future releases, so we recommend that users
migrate to the new names soon. Additionally, we note that {\F} users, however,
may need to enlarge their \id{iout} array of optional integer outputs, and
update the indices that they query for certain linear-solver-related
statistics.

The names of all constructor routines for {\sundials}-provided
{\sunlinsol} implementations have been updated to follow the naming convention
\id{SUNLinSol\_*} where \id{*} is the name of the linear solver. The new names
are
\id{SUNLinSol\_Band},
\id{SUNLinSol\_Dense},
\id{SUNLinSol\_KLU},
\id{SUNLinSol\_LapackBand},\newline
\id{SUNLinSol\_LapackDense},
\id{SUNLinSol\_PCG},
\id{SUNLinSol\_SPBCGS},
\id{SUNLinSol\_SPFGMR},
\id{SUNLinSol\_SPGMR},
\id{SUNLinSol\_SPTFQMR}, and
\id{SUNLinSol\_SuperLUMT}.  Solver-specific ``set'' routine names have
been similarly standardized.  To minimize challenges in user migration
to the new names, the previous routine names may still be used; these
will be deprecated in future releases, so we recommend that users
migrate to the new names soon. All {\cvode} example programs and the standalone
linear solver examples have been updated to use the new naming convention.

The \id{SUNBandMatrix} constructor has been simplified to remove the
storage upper bandwidth argument.

{\sundials} integrators have been updated to utilize generic nonlinear solver
modules defined through the {\sunnonlinsol} API. This API will ease the addition
of new nonlinear solver options and allow for external or user-supplied
nonlinear solvers. The {\sunnonlinsol} API and {\sundials} provided modules are
described in Chapter~\ref{c:sunnonlinsol} and follow the same object oriented
design and implementation used by the {\nvector}, {\sunmatrix}, and {\sunlinsol}
modules. Currently two {\sunnonlinsol} implementations are provided,
{\sunnonlinsolnewton} and {\sunnonlinsolfixedpoint}. These replicate the
previous integrator specific implementations of a Newton iteration and a
fixed-point iteration (previously referred to as a functional iteration),
respectively. Note the {\sunnonlinsolfixedpoint} module can optionally utilize
Anderson's method to accelerate convergence. Example programs using each of
these nonlinear solver modules in a standalone manner have been added and all
{\cvode} example programs have been updated to use generic {\sunnonlinsol}
modules.

% CVODE specific changes due to the NLS API
With the introduction of {\sunnonlinsol} modules, the input parameter \id{iter}
to \id{CVodeCreate} has been removed along with the function
\id{CVodeSetIterType} and the constants \id{CV\_NEWTON} and \id{CV\_FUNCTIONAL}.
Similarly, the \id{ITMETH} parameter has been removed from the Fortran interface
function \id{FCVMALLOC}. Instead of specifying the nonlinear iteration type when
creating the {\cvode} memory structure, {\cvode} uses the {\sunnonlinsolnewton}
module implementation of a Newton iteration by default. For details on using a
non-default or user-supplied nonlinear solver see Chapter~\ref{s:simulation}.
{\cvode} functions for setting the nonlinear solver options (e.g.,
\id{CVodeSetMaxNonlinIters}) or getting nonlinear solver statistics (e.g.,
\id{CVodeGetNumNonlinSolvIters}) remain unchanged and internally call generic
{\sunnonlinsol} functions as needed.

Three fused vector operations and seven vector array operations have been added
to the {\nvector} API. These \textit{optional} operations are disabled by
default and may be activated by calling vector specific routines after creating
an {\nvector} (see Chapter \ref{s:nvector} for more details). The new operations
are intended to increase data reuse in vector operations, reduce parallel
communication on distributed memory systems, and lower the number of kernel
launches on systems with accelerators. The fused operations are
\id{N\_VLinearCombination},
\id{N\_VScaleAddMulti}, and
\id{N\_VDotProdMulti}
and the vector array operations are
\id{N\_VLinearCombinationVectorArray},
\id{N\_VScaleVectorArray},
\id{N\_VConstVectorArray},
\id{N\_VWrmsNormVectorArray},
\id{N\_VWrmsNormMaskVectorArray},
\id{N\_VScaleAddMultiVectorArray}, and\\
\id{N\_VLinearCombinationVectorArray}.
If an {\nvector} implementation defines any of these operations as \id{NULL},
then standard {\nvector} operations will automatically be called as necessary to
complete the computation.
\\
\\
\noindent Multiple updates to {\nveccuda} were made:
\begin{itemize}
  \item Changed \id{N\_VGetLength\_Cuda} to return the global vector length
        instead of the local vector length.
  \item Added \id{N\_VGetLocalLength\_Cuda} to return the local vector length.
  \item Added \id{N\_VGetMPIComm\_Cuda} to return the MPI communicator used.
  \item Removed the accessor functions in the namespace suncudavec.
  \item Changed the \id{N\_VMake\_Cuda} function to take a host data pointer and a device
        data pointer instead of an \id{N\_VectorContent\_Cuda} object.
  \item Added the ability to set the \id{cudaStream\_t} used for execution of the
        {\nveccuda} kernels. See the function \id{N\_VSetCudaStreams\_Cuda}.
  \item Added \id{N\_VNewManaged\_Cuda}, \id{N\_VMakeManaged\_Cuda}, and
        \id{N\_VIsManagedMemory\_Cuda} functions to accommodate using managed
        memory with the {\nveccuda}.
\end{itemize}
%
Multiple changes to {\nvecraja} were made:
\begin{itemize}
  \item Changed \id{N\_VGetLength\_Raja} to return the global vector length
        instead of the local vector length.
  \item Added \id{N\_VGetLocalLength\_Raja} to return the local vector length.
  \item Added \id{N\_VGetMPIComm\_Raja} to return the MPI communicator used.
  \item Removed the accessor functions in the namespace suncudavec.
\end{itemize}
A new {\nvector} implementation for leveraging OpenMP 4.5+ device offloading has
been added, {\nvecopenmpdev}. See \S\ref{ss:nvec_openmpdev} for more details.
\\
\\
\noindent Two changes were made in the {\cvode}/{\cvodes}/{\arkode} initial step
size algorithm:
\begin{enumerate}
  \item Fixed an efficiency bug where an extra call to the right hand side
        function was made.
  \item Changed the behavior of the algorithm if the max-iterations case is hit.
        Before the algorithm would exit with the step size calculated on the
        penultimate iteration. Now it will exit with the step size calculated
        on the final iteration.
\end{enumerate}

\noindent A {\F} 2003 interface to {\cvode} has been added along with {\F} 2003
interfaces to the following shared {\sundials} modules:
\begin{itemize}
  \item {\sunnonlinsolfixedpoint} and {\sunnonlinsolnewton} nonlinear solver modules
  \item {\sunlinsoldense}, {\sunlinsolband}, {\sunlinsolklu},
    {\sunlinsolpcg}, {\sunlinsolspbcgs}, {\sunlinsolspfgmr},
    {\sunlinsolspgmr}, and {\sunlinsolsptfqmr} linear solver modules
  \item {\nvecs}, {\nvecpthreads}, and {\nvecopenmp} vector modules
\end{itemize}


\subsection*{Changes in v3.2.1}

The changes in this minor release include the following:
\begin{itemize}
\item Fixed a bug in the {\cuda} {\nvector} where the \id{N\_VInvTest} operation
  could write beyond the allocated vector data.
\item Fixed library installation path for multiarch systems. This fix changes the default
  library installation path to \id{CMAKE\_INSTALL\_PREFIX/CMAKE\_INSTALL\_LIBDIR}
  from \id{CMAKE\_INSTALL\_PREFIX/lib}.\\
  \id{CMAKE\_INSTALL\_LIBDIR} is automatically
  set, but is available as a CMake option that can modified.
\end{itemize}

\subsection*{Changes in v3.2.0}

Support for optional inequality constraints on individual components of the
solution vector has been added to {\cvode} and {\cvodes}. See Chapter
\ref{s:math} and the description of \id{CVodeSetConstraints} in
\S\ref{sss:optin_main} for more details. Use of \id{CVodeSetConstraints}
requires the {\nvector} operations \id{N\_MinQuotient}, \id{N\_VConstrMask}, and
\id{N\_VCompare} that were not previously required by {\cvode} and {\cvodes}.
\\
\\
\noindent Fixed a problem with setting \id{sunindextype} which would occur
with some compilers (e.g. armclang) that did not define \id{\_\_STDC\_VERSION\_\_}.
\\
\\
\noindent Added hybrid MPI/CUDA and MPI/RAJA vectors to allow use of more
than one MPI rank when using a GPU system.  The vectors assume one GPU
device per MPI rank.
\\
\\
\noindent Changed the name of the {\raja} {\nvector} library to
\id{libsundials\_nveccudaraja.lib} from \newline
\id{libsundials\_nvecraja.lib} to better reflect that we only support {\cuda}
as a backend for {\raja} currently.
\\
\\
\noindent Several changes were made to the build system:
\begin{itemize}
\item CMake 3.1.3 is now the minimum required CMake version.
\item Deprecate the behavior of the \id{SUNDIALS\_INDEX\_TYPE} CMake option and
  added the \newline
  \id{SUNDIALS\_INDEX\_SIZE} CMake option to select the \id{sunindextype}
  integer size.
\item The native CMake FindMPI module is now used to locate an MPI installation.
\item If MPI is enabled and MPI compiler wrappers are not set, the build system
  will check if \id{CMAKE\_<language>\_COMPILER} can compile MPI programs before
  trying to locate and use an MPI installation.
\item The previous options for setting MPI compiler wrappers and the executable
  for running MPI programs have been have been depreated. The new options that
  align with those used in native CMake FindMPI module are
  \id{MPI\_C\_COMPILER}, \id{MPI\_CXX\_COMPILER}, \id{MPI\_Fortran\_COMPILER},
  and \id{MPIEXEC\_EXECUTABLE}.
\item When a Fortran name-mangling scheme is needed (e.g., \id{LAPACK\_ENABLE}
  is \id{ON}) the build system will infer the scheme from the Fortran
  compiler. If a Fortran compiler is not available or the inferred or default
  scheme needs to be overridden, the advanced options
  \id{SUNDIALS\_F77\_FUNC\_CASE} and \id{SUNDIALS\_F77\_FUNC\_UNDERSCORES} can
  be used to manually set the name-mangling scheme and bypass trying to infer
  the scheme.
\item Parts of the main CMakeLists.txt file were moved to new files in the
  \id{src} and \id{example} directories to make the CMake configuration file
  structure more modular.
\end{itemize}

\subsection*{Changes in v3.1.2}

The changes in this minor release include the following:
\begin{itemize}
\item Updated the minimum required version of CMake to 2.8.12 and enabled
  using rpath by default to locate shared libraries on OSX.
\item Fixed Windows specific problem where \id{sunindextype} was not correctly
  defined when using 64-bit integers for the {\sundials} index type. On Windows
  \id{sunindextype} is now defined as the MSVC basic type \id{\_\_int64}.
\item Added sparse SUNMatrix ``Reallocate'' routine to allow specification of
  the nonzero storage.
\item Updated the KLU SUNLinearSolver module to set constants for the two
  reinitialization types, and fixed a bug in the full reinitialization
  approach where the sparse SUNMatrix pointer would go out of scope on
  some architectures.
\item Updated the ``ScaleAdd'' and ``ScaleAddI'' implementations in the
  sparse SUNMatrix module to more optimally handle the case where the
  target matrix contained sufficient storage for the sum, but had the
  wrong sparsity pattern.  The sum now occurs in-place, by performing
  the sum backwards in the existing storage.  However, it is still more
  efficient if the user-supplied Jacobian routine allocates storage for
  the sum $I+\gamma J$ manually (with zero entries if needed).
\item Added the following examples from the usage notes page of the SUNDIALS website,
  and updated them to work with SUNDIALS 3.x:
  \begin{itemize}
  \item \id{cvDisc\_dns.c}, which demonstrates using CVODE with
    discontinuous solutions or RHS.
  \item \id{cvRoberts\_dns\_negsol.c}, which illustrates the
    use of the RHS function return value to control unphysical negative
    concentrations.
  \end{itemize}
\item Changed the LICENSE install path to \id{instdir/include/sundials}.
\end{itemize}

\subsection*{Changes in v3.1.1}

The changes in this minor release include the following:
\begin{itemize}
\item Fixed a minor bug in the cvSLdet routine, where a return was missing
  in the error check for three inconsistent roots.

\item Fixed a potential memory leak in the {\spgmr} and {\spfgmr} linear
  solvers: if ``Initialize'' was called multiple times then the solver
  memory was reallocated (without being freed).

\item Updated KLU {\sunlinsol} module to use a \id{typedef} for the
  precision-specific solve function to be used (to avoid compiler
  warnings).

\item Added missing typecasts for some \id{(void*)} pointers (again, to
  avoid compiler warnings).

\item Bugfix in \id{sunmatrix\_sparse.c} where we had used \id{int}
  instead of \id{sunindextype} in one location.

\item Added missing \id{\#include <stdio.h>} in {\nvector} and {\sunmatrix}
  header files.

\item Fixed an indexing bug in the {\cuda} {\nvector} implementation of
  \id{N\_VWrmsNormMask} and revised the {\raja} {\nvector} implementation of
  \id{N\_VWrmsNormMask} to work with mask arrays using values other than zero or
  one. Replaced \id{double} with \id{realtype} in the {\raja} vector test functions.

\item Fixed compilation issue with GCC 7.3.0 and Fortran programs that do
  not require a {\sunmatrix} or {\sunlinsol} module (e.g., iterative
  linear solvers or fixed-point iteration).
\end{itemize}
In addition to the changes above, minor corrections were also made to the
example programs, build system, and user documentation.

\subsection*{Changes in v3.1.0}

Added {\nvector} print functions that write vector data to a specified
file (e.g., \id{N\_VPrintFile\_Serial}).

Added \id{make test} and \id{make test\_install} options to the build
system for testing {\sundials} after building with \id{make} and
installing with \id{make install} respectively.

\subsection*{Changes in v3.0.0}

All interfaces to matrix structures and linear solvers
have been reworked, and all example programs have been updated.
The goal of the redesign of these interfaces was to provide more encapsulation
and ease in interfacing custom linear solvers and interoperability
with linear solver libraries.
Specific changes include:
\begin{itemize}
\item Added generic SUNMATRIX module with three provided implementations:
        dense, banded and sparse.  These replicate previous SUNDIALS Dls and
        Sls matrix structures in a single object-oriented API.
\item Added example problems demonstrating use of generic SUNMATRIX modules.
\item Added generic SUNLINEARSOLVER module with eleven provided
        implementations: dense, banded, LAPACK dense, LAPACK band, KLU,
        SuperLU\_MT, SPGMR, SPBCGS, SPTFQMR, SPFGMR, PCG.  These replicate
        previous SUNDIALS generic linear solvers in a single object-oriented
        API.
\item Added example problems demonstrating use of generic SUNLINEARSOLVER
        modules.
\item Expanded package-provided direct linear solver (Dls) interfaces and
        scaled, preconditioned, iterative linear solver (Spils) interfaces
        to utilize generic SUNMATRIX and SUNLINEARSOLVER objects.
\item Removed package-specific, linear solver-specific, solver modules
        (e.g. CVDENSE, KINBAND, IDAKLU, ARKSPGMR) since their functionality
        is entirely replicated by the generic Dls/Spils interfaces and
        SUNLINEARSOLVER/SUNMATRIX modules.  The exception is CVDIAG, a
        diagonal approximate Jacobian solver available to CVODE and CVODES.
\item Converted all SUNDIALS example problems to utilize new generic
        SUNMATRIX and SUNLINEARSOLVER objects, along with updated Dls and
        Spils linear solver interfaces.
\item Added Spils interface routines to ARKode, CVODE, CVODES, IDA and
        IDAS to allow specification of a user-provided "JTSetup" routine.
        This change supports users who wish to set up data structures for
        the user-provided Jacobian-times-vector ("JTimes") routine, and
        where the cost of one JTSetup setup per Newton iteration can be
        amortized between multiple JTimes calls.
\end{itemize}

Two additional {\nvector} implementations were added -- one for
{\cuda} and one for {\raja} vectors.
These vectors are supplied to provide very basic support for running
on GPU architectures.  Users are advised that these vectors both move all data
to the GPU device upon construction, and speedup will only be realized if the
user also conducts the right-hand-side function evaluation on the device.
In addition, these vectors assume the problem fits on one GPU.
Further information about {\raja}, users are referred to th web site,
https://software.llnl.gov/RAJA/.
These additions are accompanied by additions to various interface functions
and to user documentation.

All indices for data structures were updated to a new \id{sunindextype} that
can be configured to be a 32- or 64-bit integer data index type.
\id{sunindextype} is defined to be \id{int32\_t} or \id{int64\_t} when portable types are
supported, otherwise it is defined as \id{int} or \id{long int}.
The Fortran interfaces continue to use \id{long int} for indices, except for
their sparse matrix interface that now uses the new \id{sunindextype}.
This new flexible capability for index types includes interfaces to
PETSc, hypre, SuperLU\_MT, and KLU with
either 32-bit or 64-bit capabilities depending how the user configures
{\sundials}.

To avoid potential namespace conflicts, the macros defining \id{booleantype}
values \id{TRUE} and \id{FALSE} have been changed to \id{SUNTRUE} and
\id{SUNFALSE} respectively.

Temporary vectors were removed from preconditioner setup and solve
routines for all packages.  It is assumed that all necessary data
for user-provided preconditioner operations will be allocated and
stored in user-provided data structures.

The file \id{include/sundials\_fconfig.h} was added. This file contains
{\sundials} type information for use in Fortran programs.

Added functions \id{SUNDIALSGetVersion} and \id{SUNDIALSGetVersionNumber} to
get {\sundials} release version information at runtime.

The build system was expanded to support many of the xSDK-compliant keys.
The xSDK is a movement in scientific software to provide a foundation for the
rapid and efficient production of high-quality,
sustainable extreme-scale scientific applications.  More information can
be found at, https://xsdk.info.

In addition, numerous changes were made to the build system.
These include the addition of separate \id{BLAS\_ENABLE} and \id{BLAS\_LIBRARIES}
CMake variables, additional error checking during CMake configuration,
minor bug fixes, and renaming CMake options to enable/disable examples
for greater clarity and an added option to enable/disable Fortran 77 examples.
These changes included changing \id{EXAMPLES\_ENABLE} to \id{EXAMPLES\_ENABLE\_C},
changing \id{CXX\_ENABLE} to \id{EXAMPLES\_ENABLE\_CXX}, changing \id{F90\_ENABLE} to
\id{EXAMPLES\_ENABLE\_F90}, and adding an \id{EXAMPLES\_ENABLE\_F77} option.

A bug fix was made in \id{CVodeFree} to call \id{lfree} unconditionally
(if non-NULL).

Corrections and additions were made to the examples,
to installation-related files,
and to the user documentation.


\subsection*{Changes in v2.9.0}

Two additional {\nvector} implementations were added -- one for
Hypre (parallel) ParVector vectors, and one for {\petsc} vectors.  These
additions are accompanied by additions to various interface functions
and to user documentation.

Each {\nvector} module now includes a function, \id{N\_VGetVectorID},
that returns the {\nvector} module name.

For each linear solver, the various solver performance counters are
now initialized to 0 in both the solver specification function and in
solver \id{linit} function.  This ensures that these solver counters
are initialized upon linear solver instantiation as well as at the
beginning of the problem solution.

In {\fcvode}, corrections were made to three Fortran interface
functions.  Missing Fortran interface routines were added so that
users can supply the sparse Jacobian routine when using sparse direct
solvers.

A memory leak was fixed in the banded preconditioner interface.
In addition, updates were done to return integers from linear solver
and preconditioner 'free' functions.

The Krylov linear solver Bi-CGstab was enhanced by removing a redundant
dot product.  Various additions and corrections were made to the
interfaces to the sparse solvers KLU and SuperLU\_MT, including support
for CSR format when using KLU.

New examples were added for use of the OpenMP vector and for use of
sparse direct solvers from Fortran.

Minor corrections and additions were made to the {\cvode} solver, to the
Fortran interfaces, to the examples, to installation-related files,
and to the user documentation.

\subsection*{Changes in v2.8.0}

Two major additions were made to the linear system solvers that are
available for use with the {\cvode} solver.  First, in the serial case,
an interface to the sparse direct solver KLU was added.
Second, an interface to SuperLU\_MT, the multi-threaded version of
SuperLU, was added as a thread-parallel sparse direct solver option,
to be used with the serial version of the {\nvector} module.
As part of these additions, a sparse matrix (CSC format) structure
was added to {\cvode}.

Otherwise, only relatively minor modifications were made to the
{\cvode} solver:

In \id{cvRootfind}, a minor bug was corrected, where the input
array \id{rootdir} was ignored, and a line was added to break out of
root-search loop if the initial interval size is below the tolerance
\id{ttol}.

In \id{CVLapackBand}, the line \id{smu = MIN(N-1,mu+ml)} was changed to
\id{smu = mu + ml} to correct an illegal input error for \id{DGBTRF/DGBTRS}.

In order to eliminate or minimize the differences between the sources
for private functions in {\cvode} and {\cvodes}, the names of 48
private functions were changed from \id{CV**} to \id{cv**}, and a few
other names were also changed.

Two minor bugs were fixed regarding the testing of input on the first
call to \id{CVode} -- one involving \id{tstop} and one involving the
initialization of \id{*tret}.

In order to avoid possible name conflicts, the mathematical macro
and function names \id{MIN}, \id{MAX}, \id{SQR}, \id{RAbs}, \id{RSqrt},
\id{RExp}, \id{RPowerI}, and \id{RPowerR} were changed to
\id{SUNMIN}, \id{SUNMAX}, \id{SUNSQR}, \id{SUNRabs}, \id{SUNRsqrt},
\id{SUNRexp}, \id{SRpowerI}, and \id{SUNRpowerR}, respectively.
These names occur in both the solver and in various example programs.

The example program \id{cvAdvDiff\_diag\_p} was added to illustrate
the use of \id{CVDiag} in parallel.

In the FCVODE optional input routines \id{FCVSETIIN} and \id{FCVSETRIN},
the optional fourth argument \id{key\_length} was removed, with
hardcoded key string lengths passed to all \id{strncmp} tests.

In all FCVODE examples, integer declarations were revised so that
those which must match a C type \id{long int} are declared \id{INTEGER*8},
and a comment was added about the type match.  All other integer
declarations are just \id{INTEGER}.  Corresponding minor corrections were
made to the user guide.

Two new {\nvector} modules have been added for thread-parallel computing
environments --- one for OpenMP, denoted \id{NVECTOR\_OPENMP},
and one for Pthreads, denoted \id{NVECTOR\_PTHREADS}.

With this version of {\sundials}, support and documentation of the
Autotools mode of installation is being dropped, in favor of the
CMake mode, which is considered more widely portable.

\subsection*{Changes in v2.7.0}

One significant design change was made with this release: The problem
size and its relatives, bandwidth parameters, related internal indices,
pivot arrays, and the optional output \id{lsflag} have all been
changed from type \id{int} to type \id{long int}, except for the
problem size and bandwidths in user calls to routines specifying
BLAS/LAPACK routines for the dense/band linear solvers.  The function
\id{NewIntArray} is replaced by a pair \id{NewIntArray}/\id{NewLintArray},
for \id{int} and \id{long int} arrays, respectively.

A large number of minor errors have been fixed.  Among these are the following:
In \id{CVSetTqBDF}, the logic was changed to avoid a divide by zero.
After the solver memory is created, it is set to zero before being filled.
In each linear solver interface function, the linear solver memory is
freed on an error return, and the \id{**Free} function now includes a
line setting to NULL the main memory pointer to the linear solver memory.
In the rootfinding functions \id{CVRcheck1}/\id{CVRcheck2}, when an exact
zero is found, the array \id{glo} of $g$ values at the left endpoint is
adjusted, instead of shifting the $t$ location \id{tlo} slightly.
In the installation files, we modified the treatment of the macro
SUNDIALS\_USE\_GENERIC\_MATH, so that the parameter GENERIC\_MATH\_LIB is
either defined (with no value) or not defined.

\subsection*{Changes in v2.6.0}

Two new features were added in this release: (a) a new linear solver module,
based on BLAS and LAPACK for both dense and banded matrices, and (b) an option
to specify which direction of zero-crossing is to be monitored while performing
rootfinding.

The user interface has been further refined. Some of the API changes involve:
(a) a reorganization of all linear solver modules into two families (besides
the existing family of scaled preconditioned iterative linear solvers,
the direct solvers, including the new LAPACK-based ones, were also organized
into a {\em direct} family); (b) maintaining a single pointer to user data,
optionally specified through a \id{Set}-type function; and (c) a general
streamlining of the preconditioner modules distributed with the solver.

\subsection*{Changes in v2.5.0}

The main changes in this release involve a rearrangement of the entire
{\sundials} source tree (see \S\ref{ss:sun_org}). At the user interface
level, the main impact is in the mechanism of including {\sundials} header
files which must now include the relative path (e.g. \id{\#include <cvode/cvode.h>}).
Additional changes were made to the build system: all exported header files are
now installed in separate subdirectories of the instaltion {\em include} directory.

The functions in the generic dense linear solver (\id{sundials\_dense} and
\id{sundials\_smalldense}) were modified to work for rectangular $m \times n$
matrices ($m \le n$), while the factorization and solution functions were
renamed to \id{DenseGETRF}/\id{denGETRF} and \id{DenseGETRS}/\id{denGETRS},
respectively.
The factorization and solution functions in the generic band linear solver were
renamed \id{BandGBTRF} and \id{BandGBTRS}, respectively.

\subsection*{Changes in v2.4.0}

{\cvspbcg} and {\cvsptfqmr} modules have been added to interface with the
Scaled Preconditioned Bi-CGstab ({\spbcg}) and Scaled Preconditioned
Transpose-Free Quasi-Minimal Residual ({\sptfqmr}) linear solver modules,
respectively (for details see Chapter \ref{s:simulation}). Corresponding
additions were made to the {\F} interface module {\fcvode}.
At the same time, function type names for Scaled Preconditioned Iterative
Linear Solvers were added for the user-supplied Jacobian-times-vector and
preconditioner setup and solve functions.

The deallocation functions now take as arguments the address of the respective
memory block pointer.

To reduce the possibility of conflicts, the names of all header files have
been changed by adding unique prefixes (\id{cvode\_} and \id{sundials\_}).
When using the default installation procedure, the header files are exported
under various subdirectories of the target \id{include} directory. For more
details see Appendix \ref{c:install}.

\subsection*{Changes in v2.3.0}

The user interface has been further refined. Several functions used
for setting optional inputs were combined into a single one.  An optional
user-supplied routine for setting the error weight vector was added.
Additionally, to resolve potential variable scope issues, all
SUNDIALS solvers release user data right after its use. The build
systems has been further improved to make it more robust.

\subsection*{Changes in v2.2.1}

The changes in this minor {\sundials} release affect only the build system.

\subsection*{Changes in v2.2.0}

The major changes from the previous version involve a redesign of the
user interface across the entire {\sundials} suite. We have eliminated the
mechanism of providing optional inputs and extracting optional statistics
from the solver through the \id{iopt} and \id{ropt} arrays. Instead,
{\cvode} now provides a set of routines (with prefix \id{CVodeSet})
to change the default values for various quantities controlling the
solver and a set of extraction routines (with prefix \id{CVodeGet})
to extract statistics after return from the main solver routine.
Similarly, each linear solver module provides its own set of {\id{Set}-}
and {\id{Get}-type} routines. For more details see \S\ref{ss:optional_input}
and \S\ref{ss:optional_output}.

Additionally, the interfaces to several user-supplied routines
(such as those providing Jacobians and preconditioner information)
were simplified by reducing the number
of arguments. The same information that was previously accessible
through such arguments can now be obtained through {\id{Get}-type}
functions.

The rootfinding feature was added, whereby the roots of a set of given
functions may be computed during the integration of the ODE system.

Installation of {\cvode} (and all of {\sundials}) has been completely
redesigned and is now based on configure scripts.


\section{Reading this User Guide}\label{ss:reading}

This user guide is a combination of general usage instructions.
Specific example programs are provided as a separate document.
We expect that some readers will want to
concentrate on the general instructions, while others will refer
mostly to the examples, and the organization is intended to
accommodate both styles.

There are different possible levels of usage of {\cvode}. The most
casual user, with a small IVP problem only, can get by with reading
\S\ref{ss:ivp_sol}, then Chapter \ref{s:simulation} through
\S\ref{sss:cvode} only, and looking at examples in~\cite{cvode_ex}.

In a different direction, a more expert user with an IVP problem may want to
(a) use a package preconditioner (\S\ref{ss:preconds}),
(b) supply his/her own Jacobian or preconditioner routines (\S\ref{ss:user_fct_sim}),
(c) do multiple runs of problems of the same size (\S\ref{sss:cvreinit}),
(d) supply a new {\nvector} module (Chapter \ref{s:nvector}),
(e) supply new {\sunlinsol} and/or {\sunmatrix} modules (Chapters
\ref{s:sunmatrix} and \ref{s:sunlinsol}), or even
(f) supply new {\sunnonlinsol} modules (Chapter \ref{c:sunnonlinsol}).

The structure of this document is as follows:
\begin{itemize}
\item
  In Chapter \ref{s:math}, we give short descriptions of the numerical
  methods implemented by {\cvode} for the solution of initial value
  problems for systems of ODEs, and continue with short descriptions of
  preconditioning (\S\ref{s:preconditioning}), stability limit detection
  (\S\ref{s:bdf_stab}), and rootfinding (\S\ref{ss:rootfinding}).
\item
  The following chapter describes the structure of the {\sundials} suite
  of solvers (\S\ref{ss:sun_org}) and the software organization of the {\cvode}
  solver (\S\ref{ss:cvode_org}).
\item
  Chapter \ref{s:simulation} is the main usage document for {\cvode} for
  {\CC} applications.  It includes a complete description of the user interface
  for the integration of ODE initial value problems.
\item
  In Chapter \ref{s:cvfort}, we describe the use of {\cvode} with {\F} applications.
\item
  Chapter \ref{s:nvector} gives a brief overview of the generic
  {\nvector} module shared among the various components of
  {\sundials}, and details on the {\nvector} implementations
  provided with {\sundials}.
\item
  Chapter \ref{s:sunmatrix} gives a brief overview of the generic
  {\sunmatrix} module shared among the various components of
  {\sundials}, and details on the {\sunmatrix} implementations
  provided with {\sundials}:
  a dense implementation (\S\ref{ss:sunmat_dense}),
  a banded implementation (\S\ref{ss:sunmat_band}) and
  a sparse implementation (\S\ref{ss:sunmat_sparse}).
\item
  Chapter \ref{s:sunlinsol} gives a brief overview of the generic
  {\sunlinsol} module shared among the various components of
  {\sundials}.  This chapter contains details on the {\sunlinsol}
  implementations provided with {\sundials}.
  The chapter
  also contains details on the {\sunlinsol} implementations provided
  with {\sundials} that interface with external linear solver
  libraries.
%% \item
%%   Chapter \ref{s:new_linsolv} describes the interfaces to the linear
%%   solver modules, so that a user can provide his/her own such module.
%% \item
%%   Chapter \ref{s:gen_linsolv} describes in detail the generic linear
%%   solvers shared by all {\sundials} solvers.
\item
  Chapter \ref{c:sunnonlinsol} describes the {\sunnonlinsol} API and nonlinear
  solver implementations shared among the various components of {\sundials}.
\item
  Finally, in the appendices, we provide detailed instructions for the installation
  of {\cvode}, within the structure of {\sundials} (Appendix \ref{c:install}), as well
  as a list of all the constants used for input to and output from {\cvode} functions
  (Appendix \ref{c:constants}).
\end{itemize}

Finally, the reader should be aware of the following notational conventions
in this user guide:  program listings and identifiers (such as \id{CVodeInit})
within textual explanations appear in typewriter type style;
fields in {\CC} structures (such as {\em content}) appear in italics;
and packages or modules, such as {\cvls}, are written in all capitals.
Usage and installation instructions that constitute important warnings
are marked with a triangular symbol {\warn} in the margin.

\paragraph{Acknowledgments.}
We wish to acknowledge the contributions to previous versions of the
{\cvode} and {\pvode} codes and their user guides by Scott D. Cohen~\cite{CoHi:94}
and George D. Byrne~\cite{ByHi:98}.

%---------------------------------
% License information section
\input{license_info}
%---------------------------------
