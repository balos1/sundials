% This is a shared SUNDIALS TEX file with description of
% the MPIManyVector nvector implementation
%
\section{The NVECTOR\_MPIMANYVECTOR implementation}\label{ss:nvec_mpimanyvector}

The {\nvecmpimanyvector} implementation of the {\nvector} module provided
with {\sundials} is designed to facilitate problems with an inherent
data partitioning for the solution vector, and when using
distributed-memory parallel architectures.  As such, the MPIManyVector
implementation supports all use cases allowed by the MPI-unaware
ManyVector implementation, as well as partitioning data between nodes
in a parallel environment.  These data partitions are entirely
user-defined, through construction of distinct {\nvector} modules for
each component, that are then combined together to form the
{\nvecmpimanyvector}.  We envision three generic use cases for this
implementation:
\begin{itemize}
\item[A.] \emph{Heterogeneous computational architectures (single-node or
  multi-node)}: for users who wish to partition data on a node between
  different computing resources, they may create architecture-specific
  subvectors for each partition.  For example, a user could create one
  MPI-parallel component based on {\nvecp}, another single-node
  component for GPU accelerators based on {\nveccuda}, and another
  threaded single-node component based on {\nvecopenmp}.
\item[B.] \emph{Process-based multiphysics decompositions (multi-node)}: for
  users who wish to combine separate simulations together, e.g., where
  one subvector resides on one subset of MPI processes, while another
  subvector resides on a different subset of MPI processes, and where
  the user has created a MPI \emph{intercommunicator} to connect these
  distinct process sets together.
\item[C.] \emph{Structure of arrays (SOA) data layouts (single-node or
  multi-node)}: for users who wish to create separate subvectors for
  each solution component, e.g., in a Navier-Stokes simulation they
  could have separate subvectors for density, velocities and
  pressure, which are combined together into a single
  {\nvecmpimanyvector} for the overall ``solution''.
\end{itemize}
We note that the above use cases are not mutually exclusive, and the
{\nvecmpimanyvector} implementation should support arbitrary combinations
of these cases.

The {\nvecmpimanyvector} implementation is designed to work with any
{\nvector} subvectors that implement the minimum \emph{required} set
of operations, however significant performance benefits may be
obtained when subvectors additionally implement the optional local
reduction operations listed in Table \ref{ss:nveclocalops}.

Additionally, {\nvecmpimanyvector} sets no limit on the number of
subvectors that may be attached (aside from the limitations of using
\id{sunindextype} for indexing, and standard per-node memory
limitations).  However, while this ostensibly supports subvectors
with one entry each (i.e., one subvector for each solution entry), we
anticipate that this extreme situation will hinder performance due to
non-stride-one memory accesses and increased function call overhead.
We therefore recommend a relatively coarse partitioning of the
problem, although actual performance will likely be
problem-dependent.

As a final note, in the coming years we plan to introduce additional
algebraic solvers and time integration modules that will leverage the
problem partitioning enabled by {\nvecmpimanyvector}.  However, even at
present we anticipate that users will be able to leverage such data
partitioning in their problem-defining ODE right-hand side, DAE
residual, or nonlinear solver residual functions.


% ====================================================================
\subsection{NVECTOR\_MPIMANYVECTOR structure}
\label{ss:nvec_mpimanyvector_structure}
% ====================================================================


The {\nvecmpimanyvector} implementation defines the {\em content} field
of \id{N\_Vector} to be a structure containing the MPI communicator
(or \id{MPI\_COMM\_NULL} if running on a single-node), the number of
subvectors comprising the MPIManyVector, the global length of the
MPIManyVector (including all subvectors on all MPI tasks), a pointer to
the beginning of the array of subvectors, and a boolean flag
\id{own\_data} indicating ownership of the subvectors that populate
\id{subvec\_array}.
%%
\begin{verbatim}
struct _N_VectorContent_MPIManyVector {
  MPI_Comm      comm;            /* overall MPI communicator        */
  sunindextype  num_subvectors;  /* number of vectors attached      */
  sunindextype  global_length;   /* overall mpimanyvector length    */
  N_Vector*     subvec_array;    /* pointer to N_Vector array       */
  booleantype   own_data;        /* flag indicating data ownership  */
};
\end{verbatim}
%%
%%--------------------------------------------

The header file to include when using this module is
\id{nvector\_mpimanyvector.h}. The installed module library to link against is
\id{libsundials\_nvecmpimanyvector.\textit{lib}} where \id{\em.lib} is typically
\id{.so} for shared libraries and \id{.a} for static libraries.

\warn\textbf{Note:} If {\sundials} is configured with MPI disabled, then the
MPIManyVector library will not be built.  Furthermore, any user codes
that include \id{nvector\_mpimanyvector.h} \emph{must} be compiled
using an MPI-aware compiler (whether the specific user code utilizes
MPI or not).  We note that the {\nvecmanyvector} implementation is
designed for ManyVector use cases in an MPI-unaware environment.


% ====================================================================
\subsection{NVECTOR\_MPIMANYVECTOR functions}
\label{ss:nvec_mpimanyvector_functions}
% ====================================================================

The {\nvecmpimanyvector} module implements all vector operations listed
in Tables \ref{ss:nvecops}, \ref{ss:nvecfusedops}, \ref{ss:nvecarrayops},
and \ref{ss:nveclocalops}, except for \id{N\_VGetArrayPointer},
\id{N\_VSetArrayPointer}, \id{N\_VScaleAddMultiVectorArray}, and
\id{N\_VLinearCombinationVectorArray}.  As such, this vector cannot be
used with the {\sundials} Fortran-77 interfaces, nor with the
{\sundials} direct solvers and preconditioners. Instead, the \\
{\nvecmpimanyvector} module provides functions to access subvectors,
whose data may in turn be accessed according to their {\nvector}
implementations.

The names of vector operations are obtained from those in Tables
\ref{ss:nvecops}, \ref{ss:nvecfusedops}, \ref{ss:nvecarrayops}, and
\ref{ss:nveclocalops} by appending the suffix \id{\_MPIManyVector}
(e.g. \id{N\_VDestroy\_MPIManyVector}).
The module {\nvecmpimanyvector} provides the following additional
user-callable routines:
%%--------------------------------------
\sunmodfunf{N\_VNew\_MPIManyVector}
{
  This function creates an MPIManyVector from a set of existing {\nvector}
  objects, under the requirement that all MPI-aware subvectors use the
  same MPI communicator (this is checked internally).  If none of the
  subvectors are MPI-aware, then this may equivalently be used to
  describe data partitioning within a single node.  We note that this
  routine is designed to support use cases A and C above.

  This routine will copy all \id{N\_Vector} pointers from the input
  \id{vec\_array}, so the user may modify/free that pointer array
  after calling this function.  However, this routine does \emph{not}
  allocate any new subvectors, so the underlying {\nvector} objects
  themselves should not be destroyed before the MPIManyVector that
  contains them.

  Upon successful completion, the new MPIManyVector is returned;
  otherwise this routine returns \id{NULL} (e.g., if two MPI-aware
  subvectors use different MPI communicators).

  Users of the Fortran 2003 interface to this function will first need
  to use the generic \id{N\_Vector} utility functions
  \id{N\_VNewVectorArray}, and \id{N\_VSetVecAtIndexVectorArray} to create
  the \id{N\_Vector*} argument. This is further explained in
  Chapter~\ref{sss:fortran2003_nvarrays}, and the functions are documented
  in Chapter~\ref{ss:nvecutils}.
}
{
  N\_Vector N\_VNew\_MPIManyVector(sunindextype num\_subvectors,
  \newlinefill{N\_Vector N\_VNew\_MPIManyVector}
  N\_Vector *vec\_array);
}
%%--------------------------------------
\sunmodfunf{N\_VMake\_MPIManyVector}
{
  This function creates an MPIManyVector from a set of existing {\nvector}
  objects, and a user-created MPI communicator that ``connects'' these
  subvectors.  Any MPI-aware subvectors may use different MPI
  communicators than the input \id{comm}.  We note that this routine
  is designed to support any combination of the use cases above.

  The input \id{comm} should be this user-created MPI communicator.
  This routine will internally call \id{MPI\_Comm\_dup} to create a
  copy of the input \id{comm}, so the user-supplied \id{comm} argument
  need not be retained after the call to \id{N\_VMake\_MPIManyVector}.

  If all subvectors are MPI-unaware, then the input \id{comm} argument
  should be \id{MPI\_COMM\_NULL}, although in this case, it would be
  simpler to call \Id{N\_VNew\_MPIManyVector} instead, or to just use
  the {\nvecmanyvector} module.

  This routine will copy all \id{N\_Vector} pointers from the input
  \id{vec\_array}, so the user may modify/free that pointer array
  after calling this function.  However, this routine does \emph{not}
  allocate any new subvectors, so the underlying {\nvector} objects
  themselves should not be destroyed before the MPIManyVector that
  contains them.

  Upon successful completion, the new MPIManyVector is returned;
  otherwise this routine returns \id{NULL} (e.g., if the input
  \id{vec\_array} is \id{NULL}).
}
{
  N\_Vector N\_VMake\_MPIManyVector(MPI\_Comm comm,
  sunindextype num\_subvectors,
  \newlinefill{N\_Vector N\_VMake\_MPIManyVector}
  N\_Vector *vec\_array);
}
%%--------------------------------------
\sunmodfunf{N\_VGetSubvector\_MPIManyVector}
{
  This function returns the \id{vec\_num} subvector from the {\nvector}
  array.
}
{
  N\_Vector N\_VGetSubvector\_MPIManyVector(N\_Vector v, sunindextype vec\_num);
}
%%--------------------------------------
\sunmodfunf{N\_VGetSubvectorArrayPointer\_MPIManyVector}
{
  This function returns the data array pointer for the \id{vec\_num}
  subvector from the {\nvector} array.

  If the input \id{vec\_num} is invalid, or if the subvector does not
  support the \id{N\_VGetArrayPointer} operation, then \id{NULL} is returned.
}
{
  realtype *N\_VGetSubvectorArrayPointer\_MPIManyVector(N\_Vector v, sunindextype vec\_num);
}
%%--------------------------------------
\sunmodfunf{N\_VSetSubvectorArrayPointer\_MPIManyVector}
{
  This function sets the data array pointer for the \id{vec\_num}
  subvector from the {\nvector} array.

  If the input \id{vec\_num} is invalid, or if the subvector does not
  support the \id{N\_VSetArrayPointer} operation, then this routine
  returns \id{-1}; otherwise it returns \id{0}.
}
{
  int N\_VSetSubvectorArrayPointer\_MPIManyVector(realtype *v\_data, N\_Vector v, sunindextype vec\_num);
}
%%--------------------------------------
\sunmodfunf{N\_VGetNumSubvectors\_MPIManyVector}
{
  This function returns the overall number of subvectors in the
  MPIManyVector object.
}
{
  sunindextype N\_VGetNumSubvectors\_MPIManyVector(N\_Vector v);
}
%%--------------------------------------
By default all fused and vector array operations are disabled in the {\nvecmpimanyvector}
module, except for \id{N\_VWrmsNormVectorArray} and
\id{N\_VWrmsNormMaskVectorArray}, that are enabled by default. The
following additional user-callable routines are provided to enable or
disable fused and vector array operations for a specific vector. To
ensure consistency across vectors it is recommended to first create a
vector with \id{N\_VNew\_MPIManyVector} or \id{N\_VMake\_MPIManyVector},
enable/disable the desired operations for that vector with the
functions below, and create any additional vectors from that vector
using \id{N\_VClone}. This guarantees that the new vectors will have
the same operations enabled/disabled, since cloned vectors inherit
those configuration options from the vector they are cloned from, while
vectors created with \id{N\_VNew\_MPIManyVector} and
\id{N\_VMake\_MPIManyVector} will have the default settings for the
{\nvecmpimanyvector} module.  We note that these routines \emph{do not}
call the corresponding routines on subvectors, so those should be set up
as desired \emph{before} attaching them to the MPIManyVector in
\id{N\_VNew\_MPIManyVector} or \id{N\_VMake\_MPIManyVector}.
%%--------------------------------------
\sunmodfunf{N\_VEnableFusedOps\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) all fused and
  vector array operations in the MPIManyVector. The return value is \id{0} for
  success and \id{-1} if the input vector or its \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableFusedOps\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableLinearCombination\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the linear
  combination fused operation in the MPIManyVector. The return value is \id{0} for
  success and \id{-1} if the input vector or its \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableLinearCombination\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableScaleAddMulti\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the scale and
  add a vector to multiple vectors fused operation in the MPIManyVector. The
  return value is \id{0} for success and \id{-1} if the input vector or its
  \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableScaleAddMulti\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableDotProdMulti\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the multiple
  dot products fused operation in the MPIManyVector. The return value is \id{0}
  for success and \id{-1} if the input vector or its \id{ops} structure are
  \id{NULL}.
}
{
  int N\_VEnableDotProdMulti\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableLinearSumVectorArray\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the linear sum
  operation for vector arrays in the MPIManyVector. The return value is \id{0} for
  success and \id{-1} if the input vector or its \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableLinearSumVectorArray\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableScaleVectorArray\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the scale
  operation for vector arrays in the MPIManyVector. The return value is \id{0} for
  success and \id{-1} if the input vector or its \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableScaleVectorArray\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableConstVectorArray\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the const
  operation for vector arrays in the MPIManyVector. The return value is \id{0} for
  success and \id{-1} if the input vector or its \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableConstVectorArray\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableWrmsNormVectorArray\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the WRMS norm
  operation for vector arrays in the MPIManyVector. The return value is \id{0} for
  success and \id{-1} if the input vector or its \id{ops} structure are \id{NULL}.
}
{
  int N\_VEnableWrmsNormVectorArray\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%--------------------------------------
\sunmodfunf{N\_VEnableWrmsNormMaskVectorArray\_MPIManyVector}
{
  This function enables (\id{SUNTRUE}) or disables (\id{SUNFALSE}) the masked WRMS
  norm operation for vector arrays in the MPIManyVector. The return value is
  \id{0} for success and \id{-1} if the input vector or its \id{ops} structure are
  \id{NULL}.
}
{
  int N\_VEnableWrmsNormMaskVectorArray\_MPIManyVector(N\_Vector v, booleantype tf);
}
%%
%%------------------------------------
%%
\paragraph{\bf Notes}

\begin{itemize}

\item
  {\warn}\id{N\_VNew\_MPIManyVector} and \id{N\_VMake\_MPIManyVector} set
  the field {\em own\_data} $=$ \id{SUNFALSE}.  \\
  \id{N\_VDestroy\_MPIManyVector} will not attempt to call
  \id{N\_VDestroy} on any subvectors contained in the subvector array
  for any \id{N\_Vector} with {\em own\_data} set to \id{SUNFALSE}. In
  such a case, it is the user's responsibility to deallocate the
  subvectors.

\item
  {\warn}To maximize efficiency, arithmetic vector operations in the
  {\nvecmpimanyvector} implementation that have more than one
  \id{N\_Vector} argument do not check for consistent internal
  representation of these vectors. It is the user's responsibility to
  ensure that such routines are called with \id{N\_Vector} arguments
  that were all created with the same subvector representations.

\end{itemize}
