% This is a shared SUNDIALS TEX file with description of
% the mpiplusx nvector implementation
%
\section{The NVECTOR\_MPIPLUSX implementation}\label{ss:nvec_mpiplusx}

The {\nvecmpiplusx} implementation of the {\nvector} module provided
with {\sundials} is designed to facilitate the MPI+X paradigm, where
X is some form of on-node (local) parallelism (e.g. OpenMP, CUDA).
This paradigm is becoming increasingly popular with the rise of
heterogeneous computing architectures.

The {\nvecmpiplusx} implementation is designed to work with any {\nvector} that
implements the minimum \emph{required} set of operations. However, it is not
recommended to use the {\nvecp}, {\nvecph}, {\nvecpetsc}, or {\nvectrilinos}
implementations underneath the {\nvecmpiplusx} module since they already provide
MPI capabilities.

% ====================================================================
\subsection{NVECTOR\_MPIPLUSX structure}
\label{ss:nvec_mpiplusx_structure}
% ====================================================================

The {\nvecmpiplusx} implementation is a thin wrapper around the
{\nvecmpimanyvector}. Accordingly, it adopts the same content structure
as defined in Section~\ref{ss:nvec_mpimanyvector_structure}. 

The header file to include when using this module is
\id{nvector\_mpiplusx.h}. The installed module library to link against is
\id{libsundials\_nvecmpiplusx.\textit{lib}} where \id{\em.lib} is typically
\id{.so} for shared libraries and \id{.a} for static libraries.

\warn\textbf{Note:} If {\sundials} is configured with MPI disabled, then the
mpiplusx library will not be built.  Furthermore, any user codes
that include \id{nvector\_mpiplusx.h} \emph{must} be compiled
using an MPI-aware compiler.

% ====================================================================
\subsection{NVECTOR\_MPIPLUSX functions}
\label{ss:nvec_mpiplusx_functions}
% ====================================================================

The {\nvecmpiplusx} module adopts all vector operations listed
in Tables \ref{ss:nvecops}, \ref{ss:nvecfusedops}, \ref{ss:nvecarrayops},
and \ref{ss:nveclocalops}, from the {\nvecmpimanyvector} (see section
\ref{ss:nvec_mpimanyvector_functions}) except for \id{N\_VGetArrayPointer}
and \id{N\_VSetArrayPointer}; the module provides its own implementation
of these functions that call the local vector implementations. Therefore,
the {\nvecmpiplusx} module implements all of the operations listed in the
referenced sections except for \id{N\_VScaleAddMultiVectorArray}, and
\id{N\_VLinearCombinationVectorArray}. Accordingly, it's compatibility
with the {\sundials} Fortran-77 interface, and with the {\sundials}
direct solvers and preconditioners depends on the local vector implementation.

The module {\nvecmpiplusx} provides the following additional
user-callable routines:
%%--------------------------------------
\sunmodfunf{N\_VMake\_MPIPlusX}
{
  This function creates an MPIPlusX vector from an existing local
  (i.e. on-node) {\nvector} object, and a user-created MPI communicator.

  The input \id{comm} should be this user-created MPI communicator.
  This routine will internally call \id{MPI\_Comm\_dup} to create a
  copy of the input \id{comm}, so the user-supplied \id{comm} argument
  need not be retained after the call to \id{N\_VMake\_MPIPlusX}.

  This routine will copy the \id{N\_Vector} pointer to the input
  \id{local\_vector}, so the underlying local {\nvector} object
  should not be destroyed before the mpiplusx that contains it.

  Upon successful completion, the new MPIPlusX is returned;
  otherwise this routine returns \id{NULL} (e.g., if the input
  \id{local\_vector} is \id{NULL}).
}
{
  N\_Vector N\_VMake\_MPIPlusX(MPI\_Comm comm, 
  \newlinefill{N\_Vector N\_VMake\_MPIPlusX}
  N\_Vector *local\_vector);
}
%%--------------------------------------
\sunmodfunf{N\_VGetLocalVector\_MPIPlusX}
{
  This function returns the local vector underneath the 
  the MPIPlusX {\nvector}.
}
{
  N\_Vector N\_VGetLocalVector\_MPIPlusX(N\_Vector v);
}
\sunmodfunf{N\_VGetArrayPointer\_MPIPlusX}
{
  This function returns the data array pointer for the local vector
  if the local vector implements the \id{N\_VGetArrayPointer} operation;
  otherwise it returns \id{NULL}.
}
{
  realtype* N\_VGetLocalVector\_MPIPlusX(N\_Vector v);
}
\sunmodfunf{N\_VSetArrayPointer\_MPIPlusX}
{
  This function sets the data array pointer for the local vector
  if the local vector implements the \id{N\_VSetArrayPointer} operation.
}
{
  void N\_VSetArrayPointer\_MPIPlusX(realtype *data, N\_Vector v);
}
%%--------------------------------------
The {\nvecmpiplusx} module does not implement any fused or vector array
operations. Instead users should enable/disable fused operations on the
local vector.
%%
%%------------------------------------
%%
\paragraph{\bf Notes} 
           
\begin{itemize}
                                        
\item
  {\warn}\id{N\_VMake\_MPIPlusX} sets the field {\em own\_data} $=$ \id{SUNFALSE}. \\
  and \id{N\_VDestroy\_MPIPlusX} will not call \id{N\_VDestroy} on the local
  vector. In this case, it is the user's responsibility to deallocate the local vector.

\item
  {\warn}To maximize efficiency, arithmetic vector operations in the
  {\nvecmpiplusx} implementation that have more than one
  \id{N\_Vector} argument do not check for consistent internal
  representation of these vectors. It is the user's responsibility to
  ensure that such routines are called with \id{N\_Vector} arguments
  that were all created with the same local vector representations.

\end{itemize}

